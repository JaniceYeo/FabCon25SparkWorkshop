{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a498c2-187f-41c7-bf9a-b93b6a1e0cc5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# ðŸŽ¼ **Lab 2 - Orchestrating Spark**\n",
    "In this module, we will explore how to orchestrate Spark workloads using Data Factory, Fabric Scheduler, and built in orchestator functions. Additionally, we will also explore how to use resource files to make code more modular.\n",
    "\n",
    "## ðŸŽ¯ What You'll Learn \n",
    "\n",
    "By the end of this lab, you'll gain insights into:  \n",
    "\n",
    "- Reference Notebook via ```%run```\n",
    "- Reference Notebook via ```notebookutils.notebook.run```\n",
    "- Reference multiple Notebooks via ```notebookutils.notebook.runMultiple```\n",
    "- How to use Notebook resources\n",
    "- How to add Notebooks into pipelines\n",
    "- Running Notebooks in a High Concurrency (HC) Session\n",
    "- Scheduling notebook with the Fabric Scheduler\n",
    "---\n",
    "\n",
    "**Get Ready to Code!**\n",
    "Now that you have an overview, let's get started with hands-on exercises! ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388f879-7a7a-4c17-b351-4b3600ea652b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## ðŸš§ **2.1 Create and Prepare Notebooks**\n",
    "### **2.1.2 Create Child Notebooks**\n",
    "Create two notebooks: `childNotebook1` and `childNotebook2`. You can create them manually or download and upload the prebuilt versions:\n",
    "- [childNotebook1.ipynb](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_lab_materials/childNotebook1.ipynb)\n",
    "- [childNotebook2.ipynb](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_lab_materials/childNotebook2.ipynb)\n",
    "\n",
    "<details> <summary><strong>ðŸ”‘ childNotebook1:</strong> Click to reveal code</summary>\n",
    "\n",
    "```python\n",
    "# Code cell 1, marked as parameters cell\n",
    "parameter1 = ''\n",
    "parameter2 = ''\n",
    "\n",
    "# Code cell 2\n",
    "print(f'This is child notebook with parameter1 = {parameter1}, parameter2 = {parameter2}')\n",
    "\n",
    "# Code cell 3\n",
    "# Return the function with exit value\n",
    "notebookutils.notebook.exit(f'Exit with current Notebook Name: {mssparkutils.runtime.context[\"currentNotebookName\"]}')\n",
    "\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details> <summary><strong>ðŸ”‘ childNotebook2:</strong> Click to reveal code</summary>\n",
    "\n",
    "```python\n",
    "# Code cell 1, marked as parameters cell\n",
    "input1 = ''\n",
    "input2 = ''\n",
    "\n",
    "# Code cell 2\n",
    "print(\"cell1 in childNotebook2\")\n",
    "print(f'input1 = {input1}\\ninput2 = {input2}')\n",
    "\n",
    "# Code cell 3\n",
    "# Return the function with exit value\n",
    "notebookutils.notebook.exit(f'Exit with current Notebook Name: {mssparkutils.runtime.context[\"currentNotebookName\"]}')\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095bacb-79d6-4272-b341-82dc8e5d698f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## ðŸ”— **2.2 Run and Chain Notebook**\n",
    "### **2.2.1 Inject a Notebook with _%run_**\n",
    "Use [`%run`](https://learn.microsoft.com/en-us/fabric/data-engineering/author-execute-notebook#reference-run) to inject another Notebook's code into the current session:\n",
    "\n",
    "```python\n",
    "%run childNotebook1 { 'parameter1': 'value1', 'parameter2': 'value2' }\n",
    "```\n",
    "You can also reference Python or SQL files from Notebook or Environment resource folders:\n",
    "\n",
    "```python\n",
    "%run [-b/--builtin | -e/--environment | -c/--current] script_file.py/.sql [variables ...]\n",
    "```\n",
    "\n",
    "`%run` options:\n",
    "- `-b` / `--builtin`: Built-in notebook resources\n",
    "- `-e` / `--environment`: Environment resources\n",
    "- `-c` / `--current`: Always uses the current Notebook's resources, even if the current Notebook is referenced by other Notebooks\n",
    "\n",
    "ðŸ“Œ **Challenge:** Use `run%` to run the code from **childNotebook2** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957c399-6d4b-4274-b4a0-e158cd0f207c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4605fb-6ac8-430e-8d25-6d18199d3e11",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "<details>\n",
    "  <summary><strong>ðŸ”‘ Answer:</strong> Click to reveal</summary>\n",
    "\n",
    "```python\n",
    "%run childNotebook2 { 'input1': 'foo', 'input2': 'bar' }\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1eb8fa-c341-4145-b42d-50c72be37eca",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## **2.2.2 Run a Notebook Programmatically with _notebookutils.notebook.run_**\n",
    "The [```notebookutils.notebook.run```](https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-utilities#reference-a-notebook) function references a notebook and returns its exit value. You can run nesting function calls in a notebook interactively or in a pipeline. The notebook being referenced runs on the Spark pool of the notebook that calls this function. In comparison to `%run`, this method shows up as a distinct job with a Notebook snapshot avalable in the Monitoring hub.\n",
    "\n",
    "```python\n",
    "notebookutils.notebook.run(\"notebook name\", <timeoutSeconds>, <parameterMap>, <workspaceId>)\n",
    "```\n",
    "\n",
    "![nbutils.run](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_media/Reference%20notebook%20via%20nbutils.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ebb91d-11c9-4c9b-a4af-b0f38d313a2c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### **2.2.3 Reference multi notebooks via _notebookutils.notebook.runMultiple_**\n",
    "The [`notebookutils.notebook.runMultiple`](https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-utilities#reference-run-multiple-notebooks-in-parallel) function allows you to run multiple notebooks in parallel or with a predefined DAG (directed-acyclic-graph). The API executes the child notebooks similar to high-concurrency mode as the same spark session is used so that compute resources are shared.\n",
    "\n",
    "```python\n",
    "notebookutils.notebook.runMultiple([\"NotebookSimple\", \"NotebookSimple2\"])\n",
    "```\n",
    "\n",
    "ðŸ“Œ **Challenge:** Use `runMultiple` to run both **childNotebook1** and **childNotebook2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bbad3b-8dab-436c-a2c0-a65abc0bb702",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "348c673e-394e-4927-807b-52dbbb9cc4c6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "<details>\n",
    "  <summary><strong>ðŸ”‘ Answer:</strong> Click to reveal</summary>\n",
    "\n",
    "~~~python\n",
    "exitValues = notebookutils.notebook.runMultiple([\"childNotebook1\", \"childNotebook2\"])\n",
    "print(exitValues)\n",
    "~~~\n",
    "</details>\n",
    "\n",
    "ðŸ’¡ **Tip:** you can use the `json` Python module to parse and format the exit values:\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "import json\n",
    "print(json.dumps(exitValues, indent=4))\n",
    "```\n",
    "\n",
    "![nbutils.multirun](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_media/Reference%20multi%20notebooks%20via%20nbutils.jpg?raw=true)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **2.2.3.1 Specifying a DAG for Additional Control**\n",
    "Run the below code to see an example of how you have use a DAG to control the exact sequencing and Notebook level configuration options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aec2f8-8ee9-4908-b426-1ff87cfd4a46",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "DAG = {\n",
    "    \"activities\": [\n",
    "        {\n",
    "            \"name\": \"step1\", # activity name, must be unique\n",
    "            \"path\": \"childNotebook1\", # notebook path\n",
    "            \"timeoutPerCellInSeconds\": 90, # max timeout for each cell, default to 90 seconds\n",
    "            \"args\": {\"parameter1\": \"foo\", \"parameter2\": \"bar\"}, # notebook parameters\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"step2\",\n",
    "            \"path\": \"childNotebook2\",\n",
    "            \"timeoutPerCellInSeconds\": 120,\n",
    "            \"args\": {\"input1\": \"foo\", \"input2\": \"bar\"}\n",
    "        }\n",
    "    ],\n",
    "    \"timeoutInSeconds\": 43200, # max timeout for the entire DAG, default to 12 hours\n",
    "    \"concurrency\": 50 # max number of notebooks to run concurrently, default to 50, this is limited by the number of executors in your Spark Pool.\n",
    "}\n",
    "notebookutils.notebook.runMultiple(DAG, {\"displayDAGViaGraphviz\": True})"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": null
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "language": null,
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
